from bees import profiler as p
from oslo_config import cfg
from nova.virt import arch
scheduler_group = cfg.OptGroup(name='scheduler', title='Scheduler configuration')
scheduler_opts = [cfg.IntOpt('max_attempts', default=3, min=1, deprecated_name='scheduler_max_attempts', deprecated_group='DEFAULT', help='\nThe maximum number of schedule attempts.\n\nThis is the maximum number of attempts that will be made for a given instance\nbuild/move operation. It limits the number of alternate hosts returned by the\nscheduler. When that list of hosts is exhausted, a ``MaxRetriesExceeded``\nexception is raised and the instance is set to an error state.\n\nPossible values:\n\n* A positive integer, where the integer corresponds to the max number of\n  attempts that can be made when building or moving an instance.\n'), cfg.IntOpt('discover_hosts_in_cells_interval', default=-1, min=-1, help='\nPeriodic task interval.\n\nThis value controls how often (in seconds) the scheduler should attempt\nto discover new hosts that have been added to cells. If negative (the\ndefault), no automatic discovery will occur.\n\nDeployments where compute nodes come and go frequently may want this\nenabled, where others may prefer to manually discover hosts when one\nis added to avoid any overhead from constantly checking. If enabled,\nevery time this runs, we will select any unmapped hosts out of each\ncell database on every run.\n\nPossible values:\n\n* An integer, where the integer corresponds to periodic task interval in\n  seconds. 0 uses the default interval (60 seconds). A negative value disables\n  periodic tasks.\n'), cfg.IntOpt('max_placement_results', default=1000, min=1, help='\nThe maximum number of placement results to request.\n\nThis setting determines the maximum limit on results received from the\nplacement service during a scheduling operation. It effectively limits\nthe number of hosts that may be considered for scheduling requests that\nmatch a large number of candidates.\n\nA value of 1 (the minimum) will effectively defer scheduling to the placement\nservice strictly on "will it fit" grounds. A higher value will put an upper\ncap on the number of results the scheduler will consider during the filtering\nand weighing process. Large deployments may need to set this lower than the\ntotal number of hosts available to limit memory consumption, network traffic,\netc. of the scheduler.\n\nPossible values:\n\n* An integer, where the integer corresponds to the number of placement results\n  to return.\n'), cfg.IntOpt('workers', min=0, help='\nNumber of workers for the nova-scheduler service.\n\nDefaults to the number of CPUs available.\n\nPossible values:\n\n* An integer, where the integer corresponds to the number of worker processes.\n'), cfg.BoolOpt('query_placement_for_routed_network_aggregates', default=False, help='\nEnable the scheduler to filter compute hosts affined to routed network segment\naggregates.\n\nSee https://docs.openstack.org/neutron/latest/admin/config-routed-networks.html\nfor details.\n'), cfg.BoolOpt('limit_tenants_to_placement_aggregate', default=False, help='\nRestrict tenants to specific placement aggregates.\n\nThis setting causes the scheduler to look up a host aggregate with the\nmetadata key of ``filter_tenant_id`` set to the project of an incoming\nrequest, and request results from placement be limited to that aggregate.\nMultiple tenants may be added to a single aggregate by appending a serial\nnumber to the key, such as ``filter_tenant_id:123``.\n\nThe matching aggregate UUID must be mirrored in placement for proper\noperation. If no host aggregate with the tenant id is found, or that\naggregate does not match one in placement, the result will be the same\nas not finding any suitable hosts for the request.\n\nPossible values:\n\n- A boolean value.\n\nRelated options:\n\n- ``[scheduler] placement_aggregate_required_for_tenants``\n'), cfg.BoolOpt('placement_aggregate_required_for_tenants', default=False, help='\nRequire a placement aggregate association for all tenants.\n\nThis setting, when limit_tenants_to_placement_aggregate=True, will control\nwhether or not a tenant with no aggregate affinity will be allowed to schedule\nto any available node. If aggregates are used to limit some tenants but\nnot all, then this should be False. If all tenants should be confined via\naggregate, then this should be True to prevent them from receiving unrestricted\nscheduling to any available node.\n\nPossible values:\n\n- A boolean value.\n\nRelated options:\n\n- ``[scheduler] placement_aggregate_required_for_tenants``\n'), cfg.BoolOpt('query_placement_for_availability_zone', default=False, help='\nUse placement to determine availability zones.\n\nThis setting causes the scheduler to look up a host aggregate with the\nmetadata key of `availability_zone` set to the value provided by an\nincoming request, and request results from placement be limited to that\naggregate.\n\nThe matching aggregate UUID must be mirrored in placement for proper\noperation. If no host aggregate with the `availability_zone` key is\nfound, or that aggregate does not match one in placement, the result will\nbe the same as not finding any suitable hosts.\n\nNote that if you enable this flag, you can disable the (less efficient)\nAvailabilityZoneFilter in the scheduler.\n\nPossible values:\n\n- A boolean value.\n\nRelated options:\n\n- ``[filter_scheduler] enabled_filters``\n'), cfg.BoolOpt('query_placement_for_image_type_support', default=False, help="\nUse placement to determine host support for the instance's image type.\n\nThis setting causes the scheduler to ask placement only for compute\nhosts that support the ``disk_format`` of the image used in the request.\n\nPossible values:\n\n- A boolean value.\n"), cfg.BoolOpt('enable_isolated_aggregate_filtering', default=False, help='\nRestrict use of aggregates to instances with matching metadata.\n\nThis setting allows the scheduler to restrict hosts in aggregates based on\nmatching required traits in the aggregate metadata and the instance\nflavor/image. If an aggregate is configured with a property with key\n``trait:$TRAIT_NAME`` and value ``required``, the instance flavor extra_specs\nand/or image metadata must also contain ``trait:$TRAIT_NAME=required`` to be\neligible to be scheduled to hosts in that aggregate. More technical details\nat https://docs.openstack.org/nova/latest/reference/isolate-aggregates.html\n\nPossible values:\n\n- A boolean value.\n'), cfg.BoolOpt('image_metadata_prefilter', default=False, help='\nUse placement to filter hosts based on image metadata.\n\nThis setting causes the scheduler to transform well known image metadata\nproperties into placement required traits to filter host based on image\nmetadata. This feature requires host support and is currently supported by the\nfollowing compute drivers:\n\n- ``libvirt.LibvirtDriver`` (since Ussuri (21.0.0))\n\nPossible values:\n\n- A boolean value.\n\nRelated options:\n\n- ``[compute] compute_driver``\n')]
filter_scheduler_group = cfg.OptGroup(name='filter_scheduler', title='Filter scheduler options')
filter_scheduler_opts = [cfg.IntOpt('host_subset_size', default=1, min=1, deprecated_name='scheduler_host_subset_size', deprecated_group='DEFAULT', help='\nSize of subset of best hosts selected by scheduler.\n\nNew instances will be scheduled on a host chosen randomly from a subset of the\nN best hosts, where N is the value set by this option.\n\nSetting this to a value greater than 1 will reduce the chance that multiple\nscheduler processes handling similar requests will select the same host,\ncreating a potential race condition. By selecting a host randomly from the N\nhosts that best fit the request, the chance of a conflict is reduced. However,\nthe higher you set this value, the less optimal the chosen host may be for a\ngiven request.\n\nPossible values:\n\n* An integer, where the integer corresponds to the size of a host subset.\n'), cfg.IntOpt('max_io_ops_per_host', default=8, min=0, deprecated_group='DEFAULT', help='\nThe number of instances that can be actively performing IO on a host.\n\nInstances performing IO includes those in the following states: build, resize,\nsnapshot, migrate, rescue, unshelve.\n\nNote that this setting only affects scheduling if the ``IoOpsFilter`` filter is\nenabled.\n\nPossible values:\n\n* An integer, where the integer corresponds to the max number of instances\n  that can be actively performing IO on any given host.\n\nRelated options:\n\n- ``[filter_scheduler] enabled_filters``\n'), cfg.IntOpt('max_instances_per_host', default=50, min=1, deprecated_group='DEFAULT', help="\nMaximum number of instances that can exist on a host.\n\nIf you need to limit the number of instances on any given host, set this option\nto the maximum number of instances you want to allow. The NumInstancesFilter\nand AggregateNumInstancesFilter will reject any host that has at least as many\ninstances as this option's value.\n\nNote that this setting only affects scheduling if the ``NumInstancesFilter`` or\n``AggregateNumInstancesFilter`` filter is enabled.\n\nPossible values:\n\n* An integer, where the integer corresponds to the max instances that can be\n  scheduled on a host.\n\nRelated options:\n\n- ``[filter_scheduler] enabled_filters``\n"), cfg.BoolOpt('track_instance_changes', default=True, deprecated_name='scheduler_tracks_instance_changes', deprecated_group='DEFAULT', help='\nEnable querying of individual hosts for instance information.\n\nThe scheduler may need information about the instances on a host in order to\nevaluate its filters and weighers. The most common need for this information is\nfor the (anti-)affinity filters, which need to choose a host based on the\ninstances already running on a host.\n\nIf the configured filters and weighers do not need this information, disabling\nthis option will improve performance. It may also be disabled when the tracking\noverhead proves too heavy, although this will cause classes requiring host\nusage data to query the database on each request instead.\n\n.. note::\n\n   In a multi-cell (v2) setup where the cell MQ is separated from the\n   top-level, computes cannot directly communicate with the scheduler. Thus,\n   this option cannot be enabled in that scenario. See also the\n   ``[workarounds] disable_group_policy_check_upcall`` option.\n\nRelated options:\n\n- ``[filter_scheduler] enabled_filters``\n- ``[workarounds] disable_group_policy_check_upcall``\n'), cfg.MultiStrOpt('available_filters', default=['nova.scheduler.filters.all_filters'], deprecated_name='scheduler_available_filters', deprecated_group='DEFAULT', help='\nFilters that the scheduler can use.\n\nAn unordered list of the filter classes the nova scheduler may apply.  Only the\nfilters specified in the ``[filter_scheduler] enabled_filters`` option will be\nused, but any filter appearing in that option must also be included in this\nlist.\n\nBy default, this is set to all filters that are included with nova.\n\nPossible values:\n\n* A list of zero or more strings, where each string corresponds to the name of\n  a filter that may be used for selecting a host\n\nRelated options:\n\n* ``[filter_scheduler] enabled_filters``\n'), cfg.ListOpt('enabled_filters', default=['AvailabilityZoneFilter', 'ComputeFilter', 'ComputeCapabilitiesFilter', 'ImagePropertiesFilter', 'ServerGroupAntiAffinityFilter', 'ServerGroupAffinityFilter'], deprecated_name='scheduler_default_filters', deprecated_group='DEFAULT', help='\nFilters that the scheduler will use.\n\nAn ordered list of filter class names that will be used for filtering\nhosts. These filters will be applied in the order they are listed so\nplace your most restrictive filters first to make the filtering process more\nefficient.\n\nAll of the filters in this option *must* be present in the ``[scheduler_filter]\navailable_filter`` option, or a ``SchedulerHostFilterNotFound`` exception will\nbe raised.\n\nPossible values:\n\n* A list of zero or more strings, where each string corresponds to the name of\n  a filter to be used for selecting a host\n\nRelated options:\n\n- ``[filter_scheduler] available_filters``\n'), cfg.ListOpt('weight_classes', default=['nova.scheduler.weights.all_weighers'], deprecated_name='scheduler_weight_classes', deprecated_group='DEFAULT', help='\nWeighers that the scheduler will use.\n\nOnly hosts which pass the filters are weighed. The weight for any host starts\nat 0, and the weighers order these hosts by adding to or subtracting from the\nweight assigned by the previous weigher. Weights may become negative. An\ninstance will be scheduled to one of the N most-weighted hosts, where N is\n``[filter_scheduler] host_subset_size``.\n\nBy default, this is set to all weighers that are included with Nova.\n\nPossible values:\n\n* A list of zero or more strings, where each string corresponds to the name of\n  a weigher that will be used for selecting a host\n'), cfg.FloatOpt('ram_weight_multiplier', default=1.0, deprecated_group='DEFAULT', help='\nRAM weight multipler ratio.\n\nThis option determines how hosts with more or less available RAM are weighed. A\npositive value will result in the scheduler preferring hosts with more\navailable RAM, and a negative number will result in the scheduler preferring\nhosts with less available RAM. Another way to look at it is that positive\nvalues for this option will tend to spread instances across many hosts, while\nnegative values will tend to fill up (stack) hosts as much as possible before\nscheduling to a less-used host. The absolute value, whether positive or\nnegative, controls how strong the RAM weigher is relative to other weighers.\n\nNote that this setting only affects scheduling if the ``RAMWeigher`` weigher is\nenabled.\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multipler\n  ratio for this weigher.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n'), cfg.FloatOpt('cpu_weight_multiplier', default=1.0, help='\nCPU weight multiplier ratio.\n\nMultiplier used for weighting free vCPUs. Negative numbers indicate stacking\nrather than spreading.\n\nNote that this setting only affects scheduling if the ``CPUWeigher`` weigher is\nenabled.\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multipler\n  ratio for this weigher.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n'), cfg.FloatOpt('disk_weight_multiplier', default=1.0, deprecated_group='DEFAULT', help='\nDisk weight multipler ratio.\n\nMultiplier used for weighing free disk space. Negative numbers mean to\nstack vs spread.\n\nNote that this setting only affects scheduling if the ``DiskWeigher`` weigher\nis enabled.\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multipler\n  ratio for this weigher.\n'), cfg.FloatOpt('io_ops_weight_multiplier', default=-1.0, deprecated_group='DEFAULT', help='\nIO operations weight multipler ratio.\n\nThis option determines how hosts with differing workloads are weighed. Negative\nvalues, such as the default, will result in the scheduler preferring hosts with\nlighter workloads whereas positive values will prefer hosts with heavier\nworkloads. Another way to look at it is that positive values for this option\nwill tend to schedule instances onto hosts that are already busy, while\nnegative values will tend to distribute the workload across more hosts. The\nabsolute value, whether positive or negative, controls how strong the io_ops\nweigher is relative to other weighers.\n\nNote that this setting only affects scheduling if the ``IoOpsWeigher`` weigher\nis enabled.\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multipler\n  ratio for this weigher.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n'), cfg.FloatOpt('pci_weight_multiplier', default=1.0, min=0.0, help='\nPCI device affinity weight multiplier.\n\nThe PCI device affinity weighter computes a weighting based on the number of\nPCI devices on the host and the number of PCI devices requested by the\ninstance.\n\nNote that this setting only affects scheduling if the ``PCIWeigher`` weigher\nand ``NUMATopologyFilter`` filter are enabled.\n\nPossible values:\n\n* A positive integer or float value, where the value corresponds to the\n  multiplier ratio for this weigher.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n'), cfg.FloatOpt('soft_affinity_weight_multiplier', default=1.0, min=0.0, help='\nMultiplier used for weighing hosts for group soft-affinity.\n\nNote that this setting only affects scheduling if the\n``ServerGroupSoftAffinityWeigher`` weigher is enabled.\n\nPossible values:\n\n* A non-negative integer or float value, where the value corresponds to\n  weight multiplier for hosts with group soft affinity.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n'), cfg.FloatOpt('soft_anti_affinity_weight_multiplier', default=1.0, min=0.0, help='\nMultiplier used for weighing hosts for group soft-anti-affinity.\n\nNote that this setting only affects scheduling if the\n``ServerGroupSoftAntiAffinityWeigher`` weigher is enabled.\n\nPossible values:\n\n* A non-negative integer or float value, where the value corresponds to\n  weight multiplier for hosts with group soft anti-affinity.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n'), cfg.FloatOpt('build_failure_weight_multiplier', default=1000000.0, help='\nMultiplier used for weighing hosts that have had recent build failures.\n\nThis option determines how much weight is placed on a compute node with\nrecent build failures. Build failures may indicate a failing, misconfigured,\nor otherwise ailing compute node, and avoiding it during scheduling may be\nbeneficial. The weight is inversely proportional to the number of recent\nbuild failures the compute node has experienced. This value should be\nset to some high value to offset weight given by other enabled weighers\ndue to available resources. To disable weighing compute hosts by the\nnumber of recent failures, set this to zero.\n\nNote that this setting only affects scheduling if the ``BuildFailureWeigher``\nweigher is enabled.\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multiplier\n  ratio for this weigher.\n\nRelated options:\n\n* ``[compute] consecutive_build_service_disable_threshold`` - Must be nonzero\n  for a compute to report data considered by this weigher.\n* ``[filter_scheduler] weight_classes``\n'), cfg.FloatOpt('cross_cell_move_weight_multiplier', default=1000000.0, help='\nMultiplier used for weighing hosts during a cross-cell move.\n\nThis option determines how much weight is placed on a host which is within the\nsame source cell when moving a server, for example during cross-cell resize.\nBy default, when moving an instance, the scheduler will prefer hosts within\nthe same cell since cross-cell move operations can be slower and riskier due to\nthe complicated nature of cross-cell migrations.\n\nNote that this setting only affects scheduling if the ``CrossCellWeigher``\nweigher is enabled.  If your cloud is not configured to support cross-cell\nmigrations, then this option has no effect.\n\nThe value of this configuration option can be overridden per host aggregate\nby setting the aggregate metadata key with the same name\n(``cross_cell_move_weight_multiplier``).\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multiplier\n  ratio for this weigher. Positive values mean the weigher will prefer\n  hosts within the same cell in which the instance is currently running.\n  Negative values mean the weigher will prefer hosts in *other* cells from\n  which the instance is currently running.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n'), cfg.BoolOpt('shuffle_best_same_weighed_hosts', default=False, help='\nEnable spreading the instances between hosts with the same best weight.\n\nEnabling it is beneficial for cases when ``[filter_scheduler]\nhost_subset_size`` is 1 (default), but there is a large number of hosts with\nsame maximal weight.  This scenario is common in Ironic deployments where there\nare typically many baremetal nodes with identical weights returned to the\nscheduler.  In such case enabling this option will reduce contention and\nchances for rescheduling events.  At the same time it will make the instance\npacking (even in unweighed case) less dense.\n'), cfg.StrOpt('image_properties_default_architecture', choices=arch.ALL, help='\nThe default architecture to be used when using the image properties filter.\n\nWhen using the ``ImagePropertiesFilter``, it is possible that you want to\ndefine a default architecture to make the user experience easier and avoid\nhaving something like x86_64 images landing on AARCH64 compute nodes because\nthe user did not specify the ``hw_architecture`` property in Glance.\n\nPossible values:\n\n* CPU Architectures such as x86_64, aarch64, s390x.\n'), cfg.ListOpt('isolated_images', default=[], deprecated_group='DEFAULT', help='\nList of UUIDs for images that can only be run on certain hosts.\n\nIf there is a need to restrict some images to only run on certain designated\nhosts, list those image UUIDs here.\n\nNote that this setting only affects scheduling if the ``IsolatedHostsFilter``\nfilter is enabled.\n\nPossible values:\n\n* A list of UUID strings, where each string corresponds to the UUID of an\n  image\n\nRelated options:\n\n* ``[filter_scheduler] isolated_hosts``\n* ``[filter_scheduler] restrict_isolated_hosts_to_isolated_images``\n'), cfg.ListOpt('isolated_hosts', default=[], deprecated_group='DEFAULT', help='\nList of hosts that can only run certain images.\n\nIf there is a need to restrict some images to only run on certain designated\nhosts, list those host names here.\n\nNote that this setting only affects scheduling if the ``IsolatedHostsFilter``\nfilter is enabled.\n\nPossible values:\n\n* A list of strings, where each string corresponds to the name of a host\n\nRelated options:\n\n* ``[filter_scheduler] isolated_images``\n* ``[filter_scheduler] restrict_isolated_hosts_to_isolated_images``\n'), cfg.BoolOpt('restrict_isolated_hosts_to_isolated_images', default=True, deprecated_group='DEFAULT', help="\nPrevent non-isolated images from being built on isolated hosts.\n\nNote that this setting only affects scheduling if the ``IsolatedHostsFilter``\nfilter is enabled. Even then, this option doesn't affect the behavior of\nrequests for isolated images, which will *always* be restricted to isolated\nhosts.\n\nRelated options:\n\n* ``[filter_scheduler] isolated_images``\n* ``[filter_scheduler] isolated_hosts``\n"), cfg.StrOpt('aggregate_image_properties_isolation_namespace', deprecated_group='DEFAULT', help='\nImage property namespace for use in the host aggregate.\n\nImages and hosts can be configured so that certain images can only be scheduled\nto hosts in a particular aggregate. This is done with metadata values set on\nthe host aggregate that are identified by beginning with the value of this\noption. If the host is part of an aggregate with such a metadata key, the image\nin the request spec must have the value of that metadata in its properties in\norder for the scheduler to consider the host as acceptable.\n\nNote that this setting only affects scheduling if the\n``AggregateImagePropertiesIsolation`` filter is enabled.\n\nPossible values:\n\n* A string, where the string corresponds to an image property namespace\n\nRelated options:\n\n* ``[filter_scheduler] aggregate_image_properties_isolation_separator``\n'), cfg.StrOpt('aggregate_image_properties_isolation_separator', default='.', deprecated_group='DEFAULT', help='\nSeparator character(s) for image property namespace and name.\n\nWhen using the aggregate_image_properties_isolation filter, the relevant\nmetadata keys are prefixed with the namespace defined in the\naggregate_image_properties_isolation_namespace configuration option plus a\nseparator. This option defines the separator to be used.\n\nNote that this setting only affects scheduling if the\n``AggregateImagePropertiesIsolation`` filter is enabled.\n\nPossible values:\n\n* A string, where the string corresponds to an image property namespace\n  separator character\n\nRelated options:\n\n* ``[filter_scheduler] aggregate_image_properties_isolation_namespace``\n')]
metrics_group = cfg.OptGroup(name='metrics', title='Metrics parameters', help='\nConfiguration options for metrics\n\nOptions under this group allow to adjust how values assigned to metrics are\ncalculated.\n')
metrics_weight_opts = [cfg.FloatOpt('weight_multiplier', default=1.0, help='\nMultiplier used for weighing hosts based on reported metrics.\n\nWhen using metrics to weight the suitability of a host, you can use this option\nto change how the calculated weight influences the weight assigned to a host as\nfollows:\n\n* ``>1.0``: increases the effect of the metric on overall weight\n* ``1.0``: no change to the calculated weight\n* ``>0.0,<1.0``: reduces the effect of the metric on overall weight\n* ``0.0``: the metric value is ignored, and the value of the\n  ``[metrics] weight_of_unavailable`` option is returned instead\n* ``>-1.0,<0.0``: the effect is reduced and reversed\n* ``-1.0``: the effect is reversed\n* ``<-1.0``: the effect is increased proportionally and reversed\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multipler\n  ratio for this weigher.\n\nRelated options:\n\n* ``[filter_scheduler] weight_classes``\n* ``[metrics] weight_of_unavailable``\n'), cfg.ListOpt('weight_setting', default=[], help="\nMapping of metric to weight modifier.\n\nThis setting specifies the metrics to be weighed and the relative ratios for\neach metric. This should be a single string value, consisting of a series of\none or more 'name=ratio' pairs, separated by commas, where ``name`` is the name\nof the metric to be weighed, and ``ratio`` is the relative weight for that\nmetric.\n\nNote that if the ratio is set to 0, the metric value is ignored, and instead\nthe weight will be set to the value of the ``[metrics] weight_of_unavailable``\noption.\n\nAs an example, let's consider the case where this option is set to:\n\n    ``name1=1.0, name2=-1.3``\n\nThe final weight will be:\n\n    ``(name1.value * 1.0) + (name2.value * -1.3)``\n\nPossible values:\n\n* A list of zero or more key/value pairs separated by commas, where the key is\n  a string representing the name of a metric and the value is a numeric weight\n  for that metric. If any value is set to 0, the value is ignored and the\n  weight will be set to the value of the ``[metrics] weight_of_unavailable``\n  option.\n\nRelated options:\n\n* ``[metrics] weight_of_unavailable``\n"), cfg.BoolOpt('required', default=True, help='\nWhether metrics are required.\n\nThis setting determines how any unavailable metrics are treated. If this option\nis set to True, any hosts for which a metric is unavailable will raise an\nexception, so it is recommended to also use the MetricFilter to filter out\nthose hosts before weighing.\n\nPossible values:\n\n* A boolean value, where False ensures any metric being unavailable for a host\n  will set the host weight to ``[metrics] weight_of_unavailable``.\n\nRelated options:\n\n* ``[metrics] weight_of_unavailable``\n'), cfg.FloatOpt('weight_of_unavailable', default=float(-10000.0), help='\nDefault weight for unavailable metrics.\n\nWhen any of the following conditions are met, this value will be used in place\nof any actual metric value:\n\n- One of the metrics named in ``[metrics] weight_setting`` is not available for\n  a host, and the value of ``required`` is ``False``.\n- The ratio specified for a metric in ``[metrics] weight_setting`` is 0.\n- The ``[metrics] weight_multiplier`` option is set to 0.\n\nPossible values:\n\n* An integer or float value, where the value corresponds to the multipler\n  ratio for this weigher.\n\nRelated options:\n\n* ``[metrics] weight_setting``\n* ``[metrics] required``\n* ``[metrics] weight_multiplier``\n')]

@p.trace('register_opts')
def register_opts(conf):
    conf.register_group(scheduler_group)
    conf.register_opts(scheduler_opts, group=scheduler_group)
    conf.register_group(filter_scheduler_group)
    conf.register_opts(filter_scheduler_opts, group=filter_scheduler_group)
    conf.register_group(metrics_group)
    conf.register_opts(metrics_weight_opts, group=metrics_group)

@p.trace('list_opts')
def list_opts():
    return {scheduler_group: scheduler_opts, filter_scheduler_group: filter_scheduler_opts, metrics_group: metrics_weight_opts}