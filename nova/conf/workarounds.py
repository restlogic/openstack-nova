from bees import profiler as p
'The \'workarounds\' group is for very specific reasons.\n\nIf you\'re:\n\n - Working around an issue in a system tool (e.g. libvirt or qemu) where the\n   fix is in flight/discussed in that community.\n - The tool can be/is fixed in some distributions and rather than patch the\n   code those distributions can trivially set a config option to get the\n   "correct" behavior.\n\nThen this is a good place for your workaround.\n\n.. warning::\n\n  Please use with care! Document the BugID that your workaround is paired with.\n'
from oslo_config import cfg
workarounds_group = cfg.OptGroup('workarounds', title='Workaround Options', help='\nA collection of workarounds used to mitigate bugs or issues found in system\ntools (e.g. Libvirt or QEMU) or Nova itself under certain conditions. These\nshould only be enabled in exceptional circumstances. All options are linked\nagainst bug IDs, where more information on the issue can be found.\n')
ALL_OPTS = [cfg.BoolOpt('disable_rootwrap', default=False, help="\nUse sudo instead of rootwrap.\n\nAllow fallback to sudo for performance reasons.\n\nFor more information, refer to the bug report:\n\n  https://bugs.launchpad.net/nova/+bug/1415106\n\nPossible values:\n\n* True: Use sudo instead of rootwrap\n* False: Use rootwrap as usual\n\nInterdependencies to other options:\n\n* Any options that affect 'rootwrap' will be ignored.\n"), cfg.BoolOpt('disable_libvirt_livesnapshot', default=False, deprecated_for_removal=True, deprecated_since='19.0.0', deprecated_reason='\nThis option was added to work around issues with libvirt 1.2.2. We no longer\nsupport this version of libvirt, which means this workaround is no longer\nnecessary. It will be removed in a future release.\n', help='\nDisable live snapshots when using the libvirt driver.\n\nLive snapshots allow the snapshot of the disk to happen without an\ninterruption to the guest, using coordination with a guest agent to\nquiesce the filesystem.\n\nWhen using libvirt 1.2.2 live snapshots fail intermittently under load\n(likely related to concurrent libvirt/qemu operations). This config\noption provides a mechanism to disable live snapshot, in favor of cold\nsnapshot, while this is resolved. Cold snapshot causes an instance\noutage while the guest is going through the snapshotting process.\n\nFor more information, refer to the bug report:\n\n  https://bugs.launchpad.net/nova/+bug/1334398\n\nPossible values:\n\n* True: Live snapshot is disabled when using libvirt\n* False: Live snapshots are always used when snapshotting (as long as\n  there is a new enough libvirt and the backend storage supports it)\n'), cfg.BoolOpt('handle_virt_lifecycle_events', default=True, help="\nEnable handling of events emitted from compute drivers.\n\nMany compute drivers emit lifecycle events, which are events that occur when,\nfor example, an instance is starting or stopping. If the instance is going\nthrough task state changes due to an API operation, like resize, the events\nare ignored.\n\nThis is an advanced feature which allows the hypervisor to signal to the\ncompute service that an unexpected state change has occurred in an instance\nand that the instance can be shutdown automatically. Unfortunately, this can\nrace in some conditions, for example in reboot operations or when the compute\nservice or when host is rebooted (planned or due to an outage). If such races\nare common, then it is advisable to disable this feature.\n\nCare should be taken when this feature is disabled and\n'sync_power_state_interval' is set to a negative value. In this case, any\ninstances that get out of sync between the hypervisor and the Nova database\nwill have to be synchronized manually.\n\nFor more information, refer to the bug report:\nhttps://bugs.launchpad.net/bugs/1444630\n\nInterdependencies to other options:\n\n* If ``sync_power_state_interval`` is negative and this feature is disabled,\n  then instances that get out of sync between the hypervisor and the Nova\n  database will have to be synchronized manually.\n"), cfg.BoolOpt('disable_group_policy_check_upcall', default=False, help='\nDisable the server group policy check upcall in compute.\n\nIn order to detect races with server group affinity policy, the compute\nservice attempts to validate that the policy was not violated by the\nscheduler. It does this by making an upcall to the API database to list\nthe instances in the server group for one that it is booting, which violates\nour api/cell isolation goals. Eventually this will be solved by proper affinity\nguarantees in the scheduler and placement service, but until then, this late\ncheck is needed to ensure proper affinity policy.\n\nOperators that desire api/cell isolation over this check should\nenable this flag, which will avoid making that upcall from compute.\n\nRelated options:\n\n* [filter_scheduler]/track_instance_changes also relies on upcalls from the\n  compute service to the scheduler service.\n'), cfg.BoolOpt('enable_numa_live_migration', default=False, deprecated_for_removal=True, deprecated_since='20.0.0', deprecated_reason='This option was added to mitigate known issues\nwhen live migrating instances with a NUMA topology with the libvirt driver.\nThose issues are resolved in Train. Clouds using the libvirt driver and fully\nupgraded to Train support NUMA-aware live migration. This option will be\nremoved in a future release.\n', help='\nEnable live migration of instances with NUMA topologies.\n\nLive migration of instances with NUMA topologies when using the libvirt driver\nis only supported in deployments that have been fully upgraded to Train. In\nprevious versions, or in mixed Stein/Train deployments with a rolling upgrade\nin progress, live migration of instances with NUMA topologies is disabled by\ndefault when using the libvirt driver. This includes live migration of\ninstances with CPU pinning or hugepages. CPU pinning and huge page information\nfor such instances is not currently re-calculated, as noted in `bug #1289064`_.\nThis means that if instances were already present on the destination host, the\nmigrated instance could be placed on the same dedicated cores as these\ninstances or use hugepages allocated for another instance. Alternately, if the\nhost platforms were not homogeneous, the instance could be assigned to\nnon-existent cores or be inadvertently split across host NUMA nodes.\n\nDespite these known issues, there may be cases where live migration is\nnecessary. By enabling this option, operators that are aware of the issues and\nare willing to manually work around them can enable live migration support for\nthese instances.\n\nRelated options:\n\n* ``compute_driver``: Only the libvirt driver is affected.\n\n.. _bug #1289064: https://bugs.launchpad.net/nova/+bug/1289064\n'), cfg.BoolOpt('ensure_libvirt_rbd_instance_dir_cleanup', default=False, help='\nEnsure the instance directory is removed during clean up when using rbd.\n\nWhen enabled this workaround will ensure that the instance directory is always\nremoved during cleanup on hosts using ``[libvirt]/images_type=rbd``. This\navoids the following bugs with evacuation and revert resize clean up that lead\nto the instance directory remaining on the host:\n\nhttps://bugs.launchpad.net/nova/+bug/1414895\n\nhttps://bugs.launchpad.net/nova/+bug/1761062\n\nBoth of these bugs can then result in ``DestinationDiskExists`` errors being\nraised if the instances ever attempt to return to the host.\n\n.. warning:: Operators will need to ensure that the instance directory itself,\n  specified by ``[DEFAULT]/instances_path``, is not shared between computes\n  before enabling this workaround otherwise the console.log, kernels, ramdisks\n  and any additional files being used by the running instance will be lost.\n\nRelated options:\n\n* ``compute_driver`` (libvirt)\n* ``[libvirt]/images_type`` (rbd)\n* ``instances_path``\n'), cfg.BoolOpt('disable_fallback_pcpu_query', default=False, deprecated_for_removal=True, deprecated_since='20.0.0', help='\nDisable fallback request for VCPU allocations when using pinned instances.\n\nStarting in Train, compute nodes using the libvirt virt driver can report\n``PCPU`` inventory and will use this for pinned instances. The scheduler will\nautomatically translate requests using the legacy CPU pinning-related flavor\nextra specs, ``hw:cpu_policy`` and ``hw:cpu_thread_policy``, their image\nmetadata property equivalents, and the emulator threads pinning flavor extra\nspec, ``hw:emulator_threads_policy``, to new placement requests. However,\ncompute nodes require additional configuration in order to report ``PCPU``\ninventory and this configuration may not be present immediately after an\nupgrade. To ensure pinned instances can be created without this additional\nconfiguration, the scheduler will make a second request to placement for\nold-style ``VCPU``-based allocations and fallback to these allocation\ncandidates if necessary. This has a slight performance impact and is not\nnecessary on new or upgraded deployments where the new configuration has been\nset on all hosts. By setting this option, the second lookup is disabled and the\nscheduler will only request ``PCPU``-based allocations.\n'), cfg.BoolOpt('never_download_image_if_on_rbd', default=False, help='\nWhen booting from an image on a ceph-backed compute node, if the image does not\nalready reside on the ceph cluster (as would be the case if glance is\nalso using the same cluster), nova will download the image from glance and\nupload it to ceph itself. If using multiple ceph clusters, this may cause nova\nto unintentionally duplicate the image in a non-COW-able way in the local\nceph deployment, wasting space.\n\nFor more information, refer to the bug report:\n\nhttps://bugs.launchpad.net/nova/+bug/1858877\n\nEnabling this option will cause nova to *refuse* to boot an instance if it\nwould require downloading the image from glance and uploading it to ceph\nitself.\n\nRelated options:\n\n* ``compute_driver`` (libvirt)\n* ``[libvirt]/images_type`` (rbd)\n'), cfg.BoolOpt('disable_native_luksv1', default=False, deprecated_for_removal=True, deprecated_since='23.0.0', deprecated_reason='\nThe underlying performance regression within libgcrypt that prompted this\nworkaround has been resolved as of 1.8.5\n', help='\nWhen attaching encrypted LUKSv1 Cinder volumes to instances the Libvirt driver\nconfigures the encrypted disks to be natively decrypted by QEMU.\n\nA performance issue has been discovered in the libgcrypt library used by QEMU\nthat serverly limits the I/O performance in this scenario.\n\nFor more information please refer to the following bug report:\n\nRFE: hardware accelerated AES-XTS mode\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1762765\n\nEnabling this workaround option will cause Nova to use the legacy dm-crypt\nbased os-brick encryptor to decrypt the LUKSv1 volume.\n\nNote that enabling this option while using volumes that do not provide a host\nblock device such as Ceph will result in a failure to boot from or attach the\nvolume to an instance. See the ``[workarounds]/rbd_block_device`` option for a\nway to avoid this for RBD.\n\nRelated options:\n\n* ``compute_driver`` (libvirt)\n* ``rbd_block_device`` (workarounds)\n'), cfg.BoolOpt('rbd_volume_local_attach', default=False, deprecated_for_removal=True, deprecated_since='23.0.0', deprecated_reason='\nThe underlying performance regression within libgcrypt that prompted this\nworkaround has been resolved as of 1.8.5\n', help='\nAttach RBD Cinder volumes to the compute as host block devices.\n\nWhen enabled this option instructs os-brick to connect RBD volumes locally on\nthe compute host as block devices instead of natively through QEMU.\n\nThis workaround does not currently support extending attached volumes.\n\nThis can be used with the disable_native_luksv1 workaround configuration\noption to avoid the recently discovered performance issues found within the\nlibgcrypt library.\n\nThis workaround is temporary and will be removed during the W release once\nall impacted distributions have been able to update their versions of the\nlibgcrypt library.\n\nRelated options:\n\n* ``compute_driver`` (libvirt)\n* ``disable_qemu_native_luksv1`` (workarounds)\n'), cfg.BoolOpt('reserve_disk_resource_for_image_cache', default=False, help='\nIf it is set to True then the libvirt driver will reserve DISK_GB resource for\nthe images stored in the image cache. If the\n:oslo.config:option:`DEFAULT.instances_path` is on different disk partition\nthan the image cache directory then the driver will not reserve resource for\nthe cache.\n\nSuch disk reservation is done by a periodic task in the resource tracker that\nruns every :oslo.config:option:`update_resources_interval` seconds. So the\nreservation is not updated immediately when an image is cached.\n\nRelated options:\n\n* :oslo.config:option:`DEFAULT.instances_path`\n* :oslo.config:option:`image_cache.subdirectory_name`\n* :oslo.config:option:`update_resources_interval`\n')]

@p.trace('register_opts')
def register_opts(conf):
    conf.register_group(workarounds_group)
    conf.register_opts(ALL_OPTS, group=workarounds_group)

@p.trace('list_opts')
def list_opts():
    return {workarounds_group: ALL_OPTS}